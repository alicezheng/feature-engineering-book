{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = pathlib.Path('~/work').expanduser()\n",
    "DATA_DIR = PROJECT_DIR / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rstudio-pubs-static.s3.amazonaws.com/120883_c8123ff272164b2a94be097a6237150b.html\n",
    "YELP_DATA_DIR = DATA_DIR / 'yelp' / 'v6' / 'yelp_dataset_challenge_academic_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flower'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('flowers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zero'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('zeroes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stemmer'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('stemmer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sixti'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('sixties')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sixti'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('sixty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goe'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('goes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PoS tagging and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first 10 reviews\n",
    "with open(YELP_DATA_DIR / 'yelp_academic_dataset_review.json') as f:\n",
    "    review_df = pd.DataFrame([\n",
    "        json.loads(f.readline()) for i in range(10)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll walk through spaCy's functions\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preload the language model\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can create a Pandas Series of spaCy nlp variables\n",
    "doc_df = review_df['text'].apply(nlp)\n",
    "type(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_df[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_df[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got VERB VB\n",
      "a DET DT\n",
      "letter NOUN NN\n",
      "in ADP IN\n",
      "the DET DT\n",
      "mail NOUN NN\n",
      "last ADJ JJ\n",
      "week NOUN NN\n",
      "that DET WDT\n",
      "said VERB VBD\n",
      "Dr. PROPN NNP\n",
      "Goldberg PROPN NNP\n",
      "is AUX VBZ\n",
      "moving VERB VBG\n",
      "to ADP IN\n",
      "Arizona PROPN NNP\n",
      "to PART TO\n",
      "take VERB VB\n",
      "a DET DT\n",
      "new ADJ JJ\n",
      "position NOUN NN\n",
      "there ADV RB\n",
      "in ADP IN\n",
      "June PROPN NNP\n",
      ". PUNCT .\n",
      "  SPACE _SP\n",
      "He PRON PRP\n",
      "will VERB MD\n",
      "be AUX VB\n",
      "missed VERB VBN\n",
      "very ADV RB\n",
      "much ADV RB\n",
      ". PUNCT .\n",
      " \n",
      "\n",
      " SPACE _SP\n",
      "I PRON PRP\n",
      "think VERB VBP\n",
      "finding VERB VBG\n",
      "a DET DT\n",
      "new ADJ JJ\n",
      "doctor NOUN NN\n",
      "in ADP IN\n",
      "NYC PROPN NNP\n",
      "that SCONJ IN\n",
      "you PRON PRP\n",
      "actually ADV RB\n",
      "like VERB VBP\n",
      "might VERB MD\n",
      "almost ADV RB\n",
      "be AUX VB\n",
      "as ADV RB\n",
      "awful ADJ JJ\n",
      "as SCONJ IN\n",
      "trying VERB VBG\n",
      "to PART TO\n",
      "find VERB VB\n",
      "a DET DT\n",
      "date NOUN NN\n",
      "! PUNCT .\n"
     ]
    }
   ],
   "source": [
    "# spaCy gives us fine-grained parts of speech using (.pos_)\n",
    "# and coarse-grained parts of speech using (.tag_)\n",
    "for token in doc_df[4]:\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a letter, the mail, Dr. Goldberg, Arizona, a new position, June, He, I, a new doctor, NYC, you, a date]\n"
     ]
    }
   ],
   "source": [
    "# spaCy also does some basic noun chunking for us\n",
    "print([chunk for chunk in doc_df[4].noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using [TextBlob](https://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('brown')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default tagger in TextBlob uses the PatternTagger, the same as [pattern](https://www.clips.uantwerpen.be/pattern), which is fine for our example. To use the NLTK tagger, we can specify the pos_tagger when we call TextBlob. More [here](http://textblob.readthedocs.io/en/dev/advanced_usage.html#advanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The default tagger in TextBlob uses the PatternTagger, which is OK for our example.\n",
    "# You can also specify the NLTK tagger, which works better for incomplete sentences.\n",
    "blob_df = review_df['text'].apply(TextBlob)\n",
    "type(blob_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.blob.TextBlob"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(blob_df[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Got', 'NNP'),\n",
       " ('a', 'DT'),\n",
       " ('letter', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mail', 'NN'),\n",
       " ('last', 'JJ'),\n",
       " ('week', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('said', 'VBD'),\n",
       " ('Dr.', 'NNP'),\n",
       " ('Goldberg', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('moving', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('Arizona', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('take', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('position', 'NN'),\n",
       " ('there', 'RB'),\n",
       " ('in', 'IN'),\n",
       " ('June', 'NNP'),\n",
       " ('He', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('missed', 'VBN'),\n",
       " ('very', 'RB'),\n",
       " ('much', 'RB'),\n",
       " ('I', 'PRP'),\n",
       " ('think', 'VBP'),\n",
       " ('finding', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('doctor', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('NYC', 'NNP'),\n",
       " ('that', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('actually', 'RB'),\n",
       " ('like', 'IN'),\n",
       " ('might', 'MD'),\n",
       " ('almost', 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('as', 'RB'),\n",
       " ('awful', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('trying', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('find', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('date', 'NN')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_df[4].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['got', 'goldberg', 'arizona', 'new position', 'june', 'new doctor', 'nyc']\n"
     ]
    }
   ],
   "source": [
    "# blobs in TextBlob also have noun phrase extraction\n",
    "print([np for np in blob_df[4].noun_phrases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
